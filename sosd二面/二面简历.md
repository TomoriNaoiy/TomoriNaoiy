Part 1
=============

# Task1 transformer
**优势**：
1. 并行计算能力强 相较于传统的Rnn循环神经网络 使用了自注意力机制 区别于rnn的逐步计算 能够计算整个序列 因此可以实现并行计算
2. 长距离依赖建模能力强 rnn每一步的计算依赖与前一步的信息 在面对长距离的计算时 可能会遗忘过早的信息 并且会出现梯度消失/爆炸的问题 而transformer使用的多头注意力机制 对整个信息进行处理 因此对于长距离序列解决能力强
3. 泛化性好 NLP、CV、语音等等领域都可以使用
### 最大优势：并行计算能力强
相对于rnn长距离计算上的缺陷 在rnn基础上增加的lstm 加入了各种门 能够一定程度上解决长距离处理上的缺陷 而transformer不仅在长距离计算上颇具优势 而且他的强大的并行计算能力才是真正让他成为热门框架 抛弃了逐步计算的方式，可以一次性并行处理整个序列。这一点对训练效率的提升是量级上的飞跃，尤其在大数据和 GPU/TPU 训练时代显得极其重要。正是因为并行计算，Transformer 才能在可接受的时间内训练超大规模模型（BERT、GPT、PaLM 等）。

**缺点**：
1. 计算复杂度高：自注意力是 O（n²）序列越长计算越慢。
2. 资源消耗大：训练需要海量数据和算力，小规模训练容易过拟合。
### 最大缺点 ：计算复杂度高
由于O（n²）的时间空间复杂度 使其在推理的过程 速度会很慢 而且这是自注意力机制最本质的缺点 需要大量数据的缺点可以通过预训练以及迁移学习等方式解决 但是时间复杂度的问题是transformer最本质的问题 这也是将来需要的优化方向

# task2 科研的工具和方法
1. 追寻业界热点：关注顶会顶刊/关注arXiv论文（arXiv Sanity Preserver）/paper with code/知乎/github
2. 学习文献阅读： 初读（理解做什么） 精度（抓细节） 选读 （重点关注实验结果） 整理核心和重点note 工具：Zotero + Zotfile + Notion/Obsidian
3. 文献管理：建立文献共享库 工具：Zotero
4. 组会讨论：个人汇报 众人讨论 提出批评或建议 工具：飞书/腾讯会议
5. 科研调研 围绕某个方向 进行方法分析 对比 分类 并找出优缺点以及优化方向
工具：思维导图（XMind）
6. 反思现有实现：不止跑论文代码 同时尝试消融实验（思考：“如果去掉某个模块会怎样？”“这个超参数为什么这样设置？”） 验证论文的claim 工具：TensorBoard
7. 产生验证idea： 从复现中找到论文的盲点或数据集的局限 提出修改建议 先做小数据集的实验 看看是否进行深入实验 工具：Colab/jupyter notebook

我觉得目前很多组常见的问题是“读论文多，但积累和共享不足”。我建议可以在组里建立一个共享文献库、复现实验数据库，甚至定期搞“idea 挑战”，让科研流程更系统化和高效。同时也可以让新加入的更快的更上进度和更好的学习


# task3
学习任务 包括部分作业和学习笔记/理解 放在我的仓库里面
- [shell note](https://github.com/TomoriNaoiy/TomoriNaoiy/blob/main/sosd%E4%BA%8C%E9%9D%A2/shell.md)
- [vim note](https://github.com/TomoriNaoiy/TomoriNaoiy/blob/main/sosd%E4%BA%8C%E9%9D%A2/vimt.md)
- [git note](https://github.com/TomoriNaoiy/TomoriNaoiy/blob/main/sosd%E4%BA%8C%E9%9D%A2/git.md)
- [命令行环境 note](https://github.com/TomoriNaoiy/TomoriNaoiy/blob/main/sosd%E4%BA%8C%E9%9D%A2/%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%8E%AF%E5%A2%83.md)
- [调试 note](https://github.com/TomoriNaoiy/TomoriNaoiy/blob/main/sosd%E4%BA%8C%E9%9D%A2/%E8%B0%83%E8%AF%95.md)

在wsl上面对机器改名或者添加新用户

<img width="1130" height="195" alt="image" src="https://github.com/user-attachments/assets/790e4d01-05ce-40c8-8df6-5d97c156e0ba" />
这里选择添加新用户 由于使用纯数字会被卡 所以使用名字+学号

<img width="678" height="85" alt="image" src="https://github.com/user-attachments/assets/4d5c21fa-80fd-484b-bfec-40c306e8a9d3" />
成功进入该用户

# task4
1.环境配置报告
- Driver Version:566.24
- CUDA Version:12.7
- CUDA Toolkit:12.6.2
- pytorch:2.8.0
2. 遇到的问题
由于cuda和driver都已经安装好了 遇到的最大问题是安装cuda toolkit的时候 一直在elight visual studio的安装失败 试了好多个版本依旧不行 后面没找了 自定义安装中取消了这个任务的安装 就成功了
3. 成果截图
<img width="1543" height="464" alt="b778f7dbd42b0a2acb083e3624cb3d2b" src="https://github.com/user-attachments/assets/bb0646d2-ced3-49cf-bbc5-48760c917291" />
------------------------------------------------------------------------------------------

# part 2
<img width="1666" height="518" alt="image" src="https://github.com/user-attachments/assets/0957ae6a-919f-4b34-a854-d24a8dce9dad" />

- 问题1： 作者试图质疑的是 面对seq2seq的问题 可以使用自注意力机制来打破传统的rnn（递归）和cnn（卷积）的建模
- 问题2： 卷积和递归（循环）
- 问题3： 完全摒弃递归和卷积 完全基于自注意力机制来建模 并且不但能够同时使用整个句子 而且通过多头注意力机制能够实现并行计算


<img width="1744" height="1150" alt="image" src="https://github.com/user-attachments/assets/8f86f186-270f-4482-bb12-5a88fcc82aeb" />

- 问题1： 由于循环神经网络架构的训练和推理过程需要一个词一个词计算 而且每一个ht都依赖于上一个ht-1以及当前的t 因此无法做到并行计算 而且在面对长距离序列的时候 不仅可能对过早的信息重要性进行稀释或者遗忘 同时还会有时间过长以及梯度丢失/爆炸等等缺点
- 问题2： 更强的并行化能力（因为不同位置的自注意力可以并行计算）；2. 显著减少训练时间（在相同硬件上训练更快），同时在质量上也能超过此前最优模型。
- 问题3：<img width="1164" height="259" alt="image" src="https://github.com/user-attachments/assets/04045d4f-11f2-422a-a851-83635e640c82" />

<img width="1624" height="592" alt="image" src="https://github.com/user-attachments/assets/d929ad62-59bd-44ab-a9a9-549b518802ba" />
- 问题1 以LSTM以及各种rnns为主的循环序列模式以及以卷积为基础的各种变体的序列模型是当时最为先进的模型 也是当时的技术基线
- 问题2 以往的注意力机制是和cnn/rnn共同使用 或说是其中的一部分 但是transformer是完全摒弃卷积和递归 完全基于自注意力机制 并且使用编码-解码架构 完全由注意力机制构成
- 问题3 ByteNet / ConvS2S 等用卷积结构并行化计算，但从任意位置到任意位置的信息传播需要多层卷积（线性或对数级路径），因此在建模远距离依赖时路径更长或需更深网络（效率/建模能力有局限）。Transformer 将任意两位置直接相连（常数路径），更利于捕捉远依赖.但是其o（n²）的复杂度是一个重要的缺点

<img width="1672" height="387" alt="image" src="https://github.com/user-attachments/assets/e20ab466-ab7b-446d-841c-44683ee804a1" />

### 问题1 使用encoder-decoder架构
Encoder：由若干（N）个相同的层堆叠，每层包含：

Multi-Head self-attention 子层

Position-wise Feed-Forward 子层
每个子层外包 Residual + LayerNorm。


Decoder：也是若干（N）个相同层堆叠，每层包含：

Masked Multi-Head self-attention（防止未来信息泄露）

Encoder-Decoder（跨编码器）attention 子层（查询来自 decoder，键/值 来自 encoder）

Position-wise Feed-Forward 子层
同样有 Residual + LayerNorm。

### 问题2
```python
def scaled_dot_product_attention(Q, K, V):
"""
  计算缩放点积注意⼒
Q: Queries a.k.a 查询矩阵
K: Keys a.k.a 键矩阵
V: Values a.k.a 值矩阵
"""
d_k = K.shape[-1] # key的维度 
# 步骤1: 计算Q和K的点积
scores = np.matmul(Q, K.transpose(0, 2, 1))
# 步骤2: 进⾏缩放
scaled_scores = scores / math.sqrt(d_k)
# 步骤3: 应⽤Softmax获得注意⼒权重
attention_weights = softmax(scaled_scores, axis=-1)
output = np.matmul(attention_weights, V)
return output, attention_weights
```
### 问题3
Q/K/V 投影到多个子空间（heads），在每个子空间上并行做注意力，然后拼接。这样模型可以在不同子空间捕获不同类型的关系／特征（不同“关注角度”），避免单一注意力把多种关系平均掉，从而提高表达力；同时每个 head 用较小维度使总计算量接近单头注意力。

### 问题4
使用位置编码 便于体现词向量之间的相对距离
```python
import numpy as np
def get_positional_encoding(pos, d_model):
    pe = np.zeros(d_model)
    for i in range(d_model):
        angle = pos / (10000 ** ((2 * (i // 2)) / d_model))
        if i % 2 == 0:
            pe[i] = np.sin(angle)
        else:
            pe[i] = np.cos(angle)
    return pe
```
### 问题5
每层还有一个点式前馈 FFN 对每个位置独立且相同参数地应用（所以叫 position-wise），本质上相当于对每个位置做相同的 MLP（或两个 1×1 卷积）。
<img width="1666" height="362" alt="image" src="https://github.com/user-attachments/assets/d94bd20f-86e7-43f7-9e9f-dad961cbdb32" />
### 问题1
 在 WMT 2014 English→German 新测试上：

Transformer (base)：BLEU = 27.3；

Transformer (big)：BLEU = 28.4。
在 English→French（WMT2014）上，big 模型达 41.8 BLEU。论文指出 big 模型在 EN→DE 上超过此前最好的模型（包括若干 ensemble）超过 2 BLEU。
### 问题2
  作者还把 Transformer 成功应用到 English constituency parsing（句法分析），显示出较好的泛化能力（论文中包含相应实验和结果）
<img width="1671" height="559" alt="image" src="https://github.com/user-attachments/assets/b9a52d27-1bc3-4f7d-a7bb-35261f778a7c" />

- 问题1 论文的变体试验显示：把注意力头（h）从论文默认（例如 8）减少到 1 时，BLEU 会下降（论文提到 single-head 比最好设置差约 0.9 BLEU），这支持了 multi-head 有助于性能的结论。
- 问题2 论文的实验组（表 3 的 (B) 组）表明把 key 的维度 d_k 缩小会损害模型质量，说明 attention 中确定兼容性的函数不容易，较小的 d_k 限制了表达能力。
- 问题3 对 FLOPS 训练成本估算的批判性思考

作者用“训练时间 × GPU 数 × GPU 理论 TFLOPS”估算 FLOPS，这是一个便捷的粗略估算方法（论文里也明说了估算方式），但在比较不同团队/时期的训练成本时存在若干干扰因素：

不同实现（优化、框架、内核矩阵乘实现）与 batch 策略差异；

GPU 的 sustained throughput 与理论 TFLOPS 有差距（风扇、内存带宽、通信开销等）；

是否使用混合精度、是否有通信瓶颈、IO/预处理成本、checkpoint 策略（平均最后若干 checkpoints）等都会影响实际 FLOPS/时间；

ensemble 与单模型之间的可比性问题。
因此论文给出的估算能作为一个大致参考（同硬件/实现时很有价值），但跨团队跨时代做绝对公平比较时要谨慎。
<img width="1668" height="778" alt="image" src="https://github.com/user-attachments/assets/db822256-4354-4b39-af65-b409b51466ee" />
1. 问题1 把 Transformer 扩展到除文本以外的输入/输出模态（如图像、音频、视频）。

研究局部/受限注意力（local/restricted attention）以更高效地处理很长的输入/输出（减小 o（n²）
) 的瓶颈），以及让生成阶段更少地依赖顺序生成

2. 问题2 单层 self-attention 的复杂度随序列长度呈 二次 o（n²），因此当处理极长序列（例如整本书、长音频/视频帧）时内存和计算都会迅速成为瓶颈。论文明确提出受限注意力（只关注局部邻域）作为一种减轻方法（复杂度变为 
O(r⋅n⋅d)）

3. 问题3 Transformer 之后，基于它的预训练与大规模自回归 / encoder-only 模型在 NLP 领域取得巨大成功：

BERT（Devlin et al., 2018）：基于 Transformer encoder 的预训练-微调范式，迅速把多项下游任务推到 SOTA。


GPT 系列（例如 GPT-3，Brown et al., 2020）：基于 Transformer decoder 的大规模自回归语言模型，通过尺度化取得突破性 few-shot 能力。


结论：作者在文末对“基于注意力的模型的未来”的预期在随后几年被迅速实现并放大（预训练、超大模型、跨模态 Transformer、Vision Transformer 等均可视为直接延续或受启发的方向）。

