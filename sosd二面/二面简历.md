# task1:transformer
优势：
1. 并行计算能力强 相较于传统的Rnn循环神经网络 使用了自注意力机制 区别于rnn的逐步计算 能够计算整个序列 因此可以实现并行计算
2. 长距离依赖建模能力强 rnn每一步的计算依赖与前一步的信息 在面对长距离的计算时 可能会遗忘过早的信息 并且会出现梯度消失/爆炸的问题 而transformer使用的多头注意力机制 对整个信息进行处理 因此对于长距离序列解决能力强
3. 泛化性好：架构灵活，NLP、CV、语音、推荐系统里都有成功应用。
### 最大优势：并行计算能力强
相对于rnn长距离计算上的缺陷 在rnn基础上增加的lstm 加入了各种门 能够一定程度上解决长距离处理上的缺陷 而transformer不仅在长距离计算上颇具优势 而且他的强大的并行计算能力才是真正让他成为热门框架 抛弃了逐步计算的方式，可以一次性并行处理整个序列。这一点对训练效率的提升是量级上的飞跃，尤其在大数据和 GPU/TPU 训练时代显得极其重要。正是因为并行计算，Transformer 才能在可接受的时间内训练超大规模模型（BERT、GPT、PaLM 等）。

缺点：
1. 计算复杂度高：自注意力是 O（n²）序列越长计算越慢。
2. 资源消耗大：训练需要海量数据和算力，小规模训练容易过拟合。
### 最大缺点 ：计算复杂度高
由于O（n²）的时间空间复杂度 使其在推理的过程 速度会很慢 而且这是自注意力机制最本质的缺点 需要大量数据的缺点可以通过预训练以及迁移学习等方式解决 但是时间复杂度的问题是transformer最本质的问题 这也是将来需要的优化方向

# task2
1. 追寻业界热点：关注顶会顶刊/关注arXiv论文（arXiv Sanity Preserver）/paper with code/知乎/github
2. 学习文献阅读： 初读（理解做什么） 精度（抓细节） 选读 （重点关注实验结果） 整理核心和重点note 工具：Zotero + Zotfile + Notion/Obsidian
3. 文献管理：建立文献共享库 工具：Zotero
4. 组会讨论：个人汇报 众人讨论 提出批评或建议 工具：飞书/腾讯会议
5. 科研调研 围绕某个方向 进行方法分析 对比 分类 并找出优缺点以及优化方向
工具：思维导图（XMind）
6. 反思现有实现：不止跑论文代码 同时尝试消融实验（思考：“如果去掉某个模块会怎样？”“这个超参数为什么这样设置？”） 验证论文的claim 工具：TensorBoard
7. 产生验证idea： 从复现中找到论文的盲点或数据集的局限 提出修改建议 先做小数据集的实验 看看是否进行深入实验 工具：Colab/jupyter notebook

我觉得目前很多组常见的问题是“读论文多，但积累和共享不足”。我建议可以在组里建立一个共享文献库、复现实验数据库，甚至定期搞“idea 挑战”，让科研流程更系统化和高效。同时也可以让新加入的更快的更上进度和更好的学习


# task3
学习任务 包括部分作业和学习笔记/理解
- [shell]
- [vim]
- [git]
- [命令行环境]
- [调试]

在wsl上面对机器改名或者添加新用户

<img width="1130" height="195" alt="image" src="https://github.com/user-attachments/assets/790e4d01-05ce-40c8-8df6-5d97c156e0ba" />
这里选择添加新用户 由于使用纯数字会被卡 所以使用名字+学号

<img width="678" height="85" alt="image" src="https://github.com/user-attachments/assets/4d5c21fa-80fd-484b-bfec-40c306e8a9d3" />
成功进入该用户


