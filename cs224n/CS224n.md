CS224n
=========
学完了cs231n 继续深造nlp
# L1
## one-hot
独热编码 一种词向量获得方式 由0和1组成 但是具有过于离散并且词语关联性不强的缺点

## word2Vec
相较于前者的升级版 使用了中心词和上下文词的方式 由中心词预测上下文 并提出了预测率 个人认为和神经网络的评分函数类似 

同样的 既然要优化 需要损失函数
<img width="551" height="140" alt="image" src="https://github.com/user-attachments/assets/d53fd732-a33a-4744-b22b-f6fb60b307cc" />
那么求导自然必不可少
<img width="843" height="264" alt="image" src="https://github.com/user-attachments/assets/9e6f3a4a-ded9-4941-8f24-685e7bf1723e" />

其实这就是skip-gram的核心算法

这种词向量的获取方式 对于nlp是非常实用且重要的
<img width="1103" height="693" alt="image" src="https://github.com/user-attachments/assets/d4e8973f-f88f-4934-8c73-5a08d8e51c58" />
如此可以看到 获得的词语能够以类相聚

更神奇的是 词向量之间甚至可以线性计算
v(king)-v(man)+v(women)得到的词向量，最相似的就是v(queen)！
# L2
上文提到skip-gram算法

那么什么是sg算法呢

其实就是在word2Vec的基础上 增加了一个窗口（window） 然后通过这个窗口大小来筛选上下文词

这篇讲义主要说了skip-gram算法的组成部分

## 对中心词的损失函数
<img width="997" height="160" alt="image" src="https://github.com/user-attachments/assets/e9fa7487-0424-4bf8-887b-3e65bb98a00f" />
由于common方法会导致求导计算量大幅升高 我们使用负采样收集的方法
<img width="1149" height="1043" alt="image" src="https://github.com/user-attachments/assets/7f37a649-c29a-43d4-85a5-8aed55a5c846" />

# 对滑动窗口的损失函数
<img width="1013" height="162" alt="image" src="https://github.com/user-attachments/assets/66320a49-2426-4f19-9641-09038200a4b4" />

总结来说 skip-gram的算法的训练结果是给出一个中心值 能够推测出上下文词 比如给出sit单词 能够预测到on 或者in等单词

# as1
完成对word2vec的逻辑以及对于相似度的认识

没什么好讲的 主要是```similar_words = wv_from_bin.most_similar(word, topn=10)```这个函数来获得相似单词

以及```pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'entertainment'], negative=['man']))```

通过类比获得单词 娱乐-女人+男人

# L3
尽管word2vec看起来非常高效了 但是在实际是使用中还是并不普遍 为何呢？

回顾一下w2v 通过将单词转化为向量 然后通过上下文和中心词进行相似度的求解 一个浅层神经网络

那么我们还有另一种简单的实现词向量的方式 **共现矩阵**

我们的做法是 统计哪些词是**经常一起出现的** 那么他们就是相似的 那么这么做就需要构建一个巨大的共现矩阵

<img width="1095" height="788" alt="image" src="https://github.com/user-attachments/assets/fc7542fc-0900-473b-9987-d683857f2aa3" />

这样就能获得共现次数 但是仍然有问题

维度=词汇量大小，还是太大了；

还是太过于稀疏，在做下游任务的时候依然不够方便。

那么又可以引入一个新的方法SVD 矩阵分解 核心思路就是进行特征筛选 只保留特征（在深度学习中 保留特征值是一个经常用到的方法 在效率的提升上很有效果）

调用的方法也很简单 以及有包装好的库函数了

<img width="1198" height="876" alt="image" src="https://github.com/user-attachments/assets/0ddec91f-b833-4c21-bb1e-8d968569e0b4" />

可见，即使这么简单的三句话构建的语料，我们通过构建共现矩阵、进行SVD降维、可视化，依然呈现出了类似Word2Vec的效果。

但是仍然有问题 共现矩阵的需求还是太大了 在小规模的的训练上 word2vec明显效率更高 那么有什么办法 能结合一下两者的优点或者再进行优化呢

## Glove词向量

一种是由Word2Vec的skip-gram算法改进而来（思路较为清晰）；

一种是由词语见的“共现概率比”构造出来（过程较为复杂）。

使用了共现矩阵 

<img width="949" height="192" alt="image" src="https://github.com/user-attachments/assets/f993005f-1e58-47c8-a674-7531792129a6" />

这样的实现 统合了两者的有点 在大部分的训练中比较具有优势 运用也更普遍

# L4 
### 两种关系
1. 组成关系（Constituency）
2. 依赖关系（Dependency）

前者就非常好理解了 组成就是研究一个句子的结构 语法什么的小学语文都学过了 主要讲讲依赖关系

依赖关系，则主要关心的是句子中的每一个词， 都依赖于哪个其他的词。 比如下面这个句子：

“瞧这个可爱的小傻瓜！”

“傻瓜”，是“瞧” 这个动作的对象，因此“傻瓜”是依赖于“瞧”的；

“可爱的”、“小” 都是修饰“傻瓜”的，因此，这两个形容词都是依赖于“ 傻瓜” 的；

“这个”同样是指示“傻瓜”的，因此它也依赖于“傻瓜” 。

这样，我们就清楚了这个句子中的所有依赖关系，画成依赖关系图则是这样：

<img width="1093" height="371" alt="image" src="https://github.com/user-attachments/assets/5740ff57-c67f-4f16-be94-59078e1e3a61" />

那么如何实现呢 通过三步

我们构造一个三元组，分别是Stack、Buffer和一个Dependency Set。

Stack最开始只存放一个Root节点；

Buffer则装有我们需要解析的一个句子；

Set中则保存我们分析出来的依赖关系， 最开始是空的。

我们要做的事情，就是不断地把Buffer中的词往Stack中推，跟Stack中的词判断是否有依 赖关系，有的话则输出到Set中，直到Buffer中的词全部推出，Stack中也仅剩一个 Root，就分析完毕了。
（其实就是栈的思路）

那么知道了这个依赖关系 对于我们nlp有什么作用呢

我们有**神经依存分析（Neural Dependency Parsing）**

首先明确，我们的预测任务，是「根据当前的状态，即Stack、Buffer、Set的当前状态，来构建特征，然后预测出下一步的动作」。

<img width="1098" height="436" alt="image" src="https://github.com/user-attachments/assets/82531e88-d50f-45ed-a59b-09dab32a7cfc" />

大致了解即可
