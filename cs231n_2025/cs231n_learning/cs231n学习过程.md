# 首先先恭喜自己完成了cs231n的全部内容
对于我这个新手来说 这门课明显难度不小 而且很多反向传播的公式（求偏导的）我并没有那个能力去求 大部分是查阅的资料 

个人感觉完成assignment之后受益匪浅（其实也就是大概明白了部分实现和基本原理） 因此写一些笔记记录一下学到的部分细节和重点 以便回顾

Assignment1
===
# K-nn
这部分比较简单 几乎算不上分类器 也就是单纯的对特征逐个比较 然后寻找”距离“最小的图片。这样的效率很低 准确率也很低 总体来说思路并不算难 重点就是代码的实现
**代码实现** 主要难点就是矢量化的实现 多使用广播机制
# SVM
要讲svm 就先讲讲线性分类器 其中包含了评分函数（score function）和损失函数（loss function）

评分函数 即是通过f（w，x，b）=Wx+b的函数对一个图片评分 然后分类
而 w是权重 类似于一个类别对于图像的各个特征的重视程度
而不同的w对于分类图像可能会有不同的效果 因此我们需要对这个评分的结果进行评价

也就是 损失函数

而 svm 便是使用其中一种损失函数进行分类的 多类支持向量机（SVM）损失函数

那么svm的损失函数如何计算？

$$
**L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)**
$$

而在实际操作中 具体函数便是

$$
L_i = \sum \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)
$$

也就是说 如果预测与实际的差高于某个值 我们便对其进行惩罚

我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。

### 正则化
上面损失函数有一个问题。假设有一个数据集和一个权重集W能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有
）。问题在于这个W并不唯一：可能有很多相似的W都能正确地分类所有的数据。一个简单的例子：如果W能够正确分类所有数据，即对于每个数据，损失值都是0。那么当
时，任何数乘
都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对W乘以2将使得差距变成30。

换句话说，我们希望能向某些特定的权重W添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个正则化惩罚（regularization penalty）
部分。最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：

$$
R(W) = \sum_{k} \sum_{l} W_{k,l}^2
$$

也就是说 最后的公式是

$$
L = \frac{1}{N} \sum_{i} L_i + \lambda R(W)
$$

### 最优化
这便是找到最优w的过程

通过公式可见，每个样本的数据损失值是以
为参数的线性函数的总和（零阈值来源于
函数）。有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。

而我们如何找到最优w呢 如果采用随机w 并不断取优 的确有一点效果 但效率极低且浪费算力

于是我们使用**随机梯度下降**
说到梯度 简单来说 就是损失函数的斜率

既然我们的目的是减小损失值（越小说明模型效果越好或者训练效果显著） 那么我们只需要根据负梯度更新参数 便可以最高效的找到w

而svm中 我们只需要对损失函数微分 就能得到梯度公式

$$
\nabla_{w_{y_{i}}} L_{i} = -\left( \sum_{j \neq y_{i}} 1(w_{j}^{T} x_{i} - w_{y_{i}}^{T} x_{i} + \Delta > 0) \right) x_{i}
$$

大概解释一下 就是非正确分类的部分 大于0就是1 小于0就是0

正确分类的部分 减去非零部分的个数和

经过这些操作 就能获得梯度 然后只需要在每次训练时向着负梯度的方向下降 就能逐渐优化W 从而减小损失值

**代码实现**在svm的py作业中 难点依旧在矢量化的实现 在显性循环的部分 只需要分行计算得分 求出margin 讨论后在分别求出损失值和dW

而在矢量化实现中 我们需要使用 **gradient[np.arange(num_train),y]=-row_num**方法区分出正确分类的部分 并使用矩阵乘法完成加减的实现

当然最麻烦的部分还是对与广播机制的使用以及对矩阵维度和大小的reshape（容易错 多看ai 反正我是这样）

# softmax
这个其实跟svm殊途同归 个人认为都近似属于单层神经网络 但softmax的命名并非是损失值的计算 而是来自于其对于评分函数的处理

**softmax**会对得到的评分进行归一化处理（我也不知道是不是这么说） 个人理解呢 就是将其转化为相对概率

比如svm得到的评分可能是[100,-1,114514]而softmax对评分进行函数处理后会变成类似于[0.1,0.0001,0.8999]

而softmax处理后 得到的dscore就会用于损失值的计算

（当然softmax还有一个特点就是需要减去np.max（f）,防止数值爆炸）

那么就是softmax的梯度了

这里的梯度就不能单纯微分得到了 而是需要求偏导了
也就是最简单的反向传播（其实就是倒推） 从得到的dscore倒推得到dW
笔记中没有给出公式 当然我也不会求偏导 于是询问ai

$$
\nabla_{\mathbf{W}} \mathcal{L}_i = -\mathbf{X}_i (\text{softmax}(\mathbf{z}_i) - \mathbf{y}_i)
$$

用人话来说 就是正确分类的部分 x[i]*(p-1) 非正确分类的部分x[i]*p

**（思路都挺简单 但我在这里矢量化的过程卡了大半天........）**

最后就是记得取平均和正则化了
# 双层神经网络
**重头戏来了**
经历了前俩的铺垫 最后难点来了 两层神经网络
其实也就是三层 input层 hidden层 以及output层

首先input输入一个数 然后进行一次线性变化 也就是Wx+B

进入hidden层

然后就是经过非线性变化
**也就是激活函数**
常见的是sigmoid函数 也就是1/1+e{-x}

但是由于其缺陷 例如梯度死亡的问题 所以我们使用  ReLU函数

这个函数炒鸡简单 就是max（0，x）

也就是只要小于0 就归0 大于0保留

那么经过激活函数后 再经过一次线性变换就是最终得分了

这个过程 从前向后推 自然就是向前传播的过程 

而损失值的计算 既然也属于这一类

但这里的损失值有一些区别 我们通过最终得分得到的损失值需要经过W1以及W2的正则化

**梯度**
这部分应该可以算是整个作业的最难点了

要求这里的梯度 很明显我们要使用反向传播 通过链式法则求偏导 一步步得到dW1以及dW2

$$
d\mathbf{W}_1 = \frac{\partial \mathcal{L}}{\partial \mathbf{W}_1} = \delta_2 \mathbf{a}_1^T
$$

$$
d\mathbf{W}_2 = \frac{\partial \mathcal{L}}{\partial \mathbf{W}_2} = \delta_3 \mathbf{a}_2^T
$$

用人话来说 就是通过偏导公式先得到dW1(正则化)

然后需要获得hidden层的dscore 然后反向通过激活函数 最后以hidden层的dscore推倒出dW1

**注意**这里的每一步都需要正则化（激活函数部分除外）

最后将梯度存入字典即可

这样以来 神经网络的大体框架就出来了

# 最后 调整超参数 
一个几乎在每个任务中都会出现的部分 实现并不难 也就是通过循环 使用不同的参数来调用solver 最终得到不同的效果

以上是完成assignment1以及部分note后的反思

Assignment2
===
今天是2月8号 终于完成了assignment2！可喜可贺 

那就趁热打铁 谈谈assignment的学习收获

# 全连接神经网络
和as1的两层神经网络的区别就是 **更多层了（**

当然没有这么浅显 全连接神经网络主打的是 **任意层**
也就是说 可以有无数层

而代码的实现中 也就是用上了循环 以循环的方式初始化W和b 唯一要注意的就是输出层的特殊性

而在loss的求值代码实现中 唯一的重点就是通过循环从而将w与系数一一对应起来 至于score 和激活函数什么的 我们早在as1就完成了函数 这里只需要调用即可

而反向传播的过程与上面不尽相同 要注意的点就是值的迭代了 （有些麻烦 卡了挺久） 

## 然后就提到了rms和adam

这两中方法实际就是对学习率的适应 根本上来说也就是提升效率的方法 

实现上也很简单 只需要跟着公式来即可 

全部完成了之后 又到了喜闻乐见的参数调优环节  经过几轮的洗礼 学习率的基本最优解便是1e-3~5e-4了

# 批量归一化 

批量归一化是一种有效的技术，用于加速深度神经网络的训练，提高模型的稳定性和性能。它通过归一化输入数据，减少内部协变量偏移，使得每一层的输入分布更加稳定。

人话来说 就是对于数据的预处理 从而提高效率

$$
d\mu = \frac{1}{N} \sum_{i=1}^{N} x_i
$$

$$
d\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2
$$

### 3. **归一化**
$$
d\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$

### 4. **缩放和偏移**
$$
d\y_i = \gamma \hat{x}_i + \beta
$$

重要的点就是要记得更新running_mean便是

反向传播公式就比较复杂了 但跟着公式一步步解决即可

然后又出现一个新概念
# Layer Normalization

实际上便是批量归一化的一种变体 批量归一化相当于是对列求平均和方差 而ln则是对行求平均和方差 相对于批量归一化 由于特征值数量大体固定 因而ln受到输入值数量的影响更小 在部分情况下更加适用

然后便是

# 随机失活（dropout）

随机失活（Dropout）是一种用于深度学习模型的正则化技术，旨在减少模型的过拟合现象。其核心思想是在训练过程中，随机地将网络中的一些神经元的输出置为零，从而减少神经元之间的依赖关系，提高模型的泛化能力。

人话来说 就是每个数据又（1-p）的概率被ban掉 换种说法就是 从x个里面只使用px个数据点 这样可以防止大量的训练数据使得其过拟合 提高泛化能力

代码的实现就非常简单了 唯一要注意的点就是结果要÷p（相当于期望从x变成了px 结果要再变回x就要除以p）

## 重中之重
# 卷积神经网络~
刚入学就听说了 如今终于是接触到了 ~~但好像也没那么牛逼~~

首先从直观上来讲 cnn比于fcn 多了卷积层和池化层

### 卷积层 
首先从功能角度来说 卷积层相当于设置了一个感受野（我个人习惯教程特性检查器） 通过这个特性检查器 对图像特征进行局部提取 
例如 

1 0 0 0 1

0 1 0 1 0

0 0 1 0 0

0 1 0 1 0

1 0 0 0 1

那么特征野假设是一个3*3的大小的

1 2 3

4 5 6

7 8 9

那么就对

1 0 0

0 1 0

0 0 1

进行点乘 然后放到一个矩阵里面 然后移倒下一格

000

101

010

然后一个个移动下去 由于有时会导致数据丢失 所以经常会对边缘进行0填充 这样就完成了卷积的过程 

代码的实现上
由于在作业中 要求的是用最直观的方式 那便使用显示循环 通过矩阵切片的方式点乘 从而完成卷积

反向传播也大体相似 只是需要将dx的填充部分切片去除掉

个人理解上 相对于线性分类器 卷积层不同于从单纯的将特征进行线性转化 而是在空间上以一个具有大小的特征检查器对特征进行一块块的提取 这样明显具有更高的效率
## 然后是最大池化
在效果上 实际就是对数据的浓缩

例如64*64大小的数据 通过2*2的最大池化后 变成了32*32

而其中提取最大值的过程 筛选掉了无用的数据 而减小了特征的大小 也提高了训练的速度

代码实现上和卷积层基本相似 只是变成取最大值的过程 以及引入了pool_height

## 然后空间归一化 和前面的批归一化同根同源 只是面对多维的数据 将特征通道移至末尾 以便于展平和求均值

# Pytorch
在这个作业中 大致讲述了pytorch的使用过程

首先是数据集的加载
```python
NUM_TRAIN = 49000
transform = T.Compose([
                T.ToTensor(),
                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
            ])

cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,
                             transform=transform)
loader_train = DataLoader(cifar10_train, batch_size=64,
                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))

cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,
                           transform=transform)
loader_val = DataLoader(cifar10_val, batch_size=64,
                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))

cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True,
                            transform=transform)
loader_test = DataLoader(cifar10_test, batch_size=64)
```
然后便是一些API
首先要定义一个展平函数 
```python
def flatten(x):
    N = x.shape[0] # read in N, C, H, W
    return x.view(N, -1)  # "flatten" the C * H * W values into a single vector per image
```
然后导入包
```
import torch.nn.functional as F
```
这是一个简单的model 包含一些卷积 激活 池化等函数 
可以通过
F.ReLU()

x=F.conv2d(x,conv_w1,conv_b1,padding=2)

等方式调用

中间的barebone就不多说了 直接讲model API
```python
class ThreeLayerConvNet(nn.Module):
    def __init__(self, in_channel, channel_1, channel_2, num_classes):
        super().__init__()
        ########################################################################
        # TODO: Set up the layers you need for a three-layer ConvNet with the  #
        # architecture defined above.                                          #
        ########################################################################
        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
        self.conv1=nn.Conv2d(in_channel,channel_1,kernel_size=5,padding=2)
        self.conv2=nn.Conv2d(channel_1,channel_2,kernel_size=3,padding=1)
        self.fc3=nn.Linear(channel_2*32*32,num_classes)
        nn.init.kaiming_normal_(self.conv1.weight)
        nn.init.kaiming_normal_(self.conv2.weight)
        nn.init.kaiming_normal_(self.fc3.weight)

        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
        ########################################################################
        #                          END OF YOUR CODE                            #
        ########################################################################

    def forward(self, x):
        scores = None
        ########################################################################
        # TODO: Implement the forward function for a 3-layer ConvNet. you      #
        # should use the layers you defined in __init__ and specify the        #
        # connectivity of those layers in forward()                            #
        ########################################################################
        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
        x=F.relu(self.conv1(x))
        x=F.relu(self.conv2(x))
        x=flatten(x)
        scores=self.fc3(x)


        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
        ########################################################################
        #                             END OF YOUR CODE                         #
        ########################################################################
        return scores


def test_ThreeLayerConvNet():
    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size [3, 32, 32]
    model = ThreeLayerConvNet(in_channel=3, channel_1=12, channel_2=8, num_classes=10)
    scores = model(x)
    print(scores.size())  # you should see [64, 10]
test_ThreeLayerConvNet()
```
先构建神经网络的类 
然后
```python
hidden_layer_size = 4000
learning_rate = 1e-2
model = TwoLayerFC(3 * 32 * 32, hidden_layer_size, 10)
optimizer = optim.SGD(model.parameters(), lr=learning_rate)

train_part34(model, optimizer)
```
便可以直接使用

# PyTorch Sequential API（最好用的一个）
```python
class Flatten(nn.Module):
    def forward(self, x):
        return flatten(x)

hidden_layer_size = 4000
learning_rate = 1e-2

model = nn.Sequential(
    Flatten(),
    nn.Linear(3 * 32 * 32, hidden_layer_size),
    nn.ReLU(),
    nn.Linear(hidden_layer_size, 10),
)

# you can use Nesterov momentum in optim.SGD
optimizer = optim.SGD(model.parameters(), lr=learning_rate,
                     momentum=0.9, nesterov=True)

train_part34(model, optimizer)

```
这是大致的实现过程

**下面是使用pytorch完成的一个目前最佳的神经网络 至于更多的细节之后再单独写一个来讲 这里直接挂出来吧**
```python


################################################################################
model = None
optimizer = None

# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
channel_1 = 32
channel_2 = 128
channel_3 = 256
learning_rate=1e-3
model=nn.Sequential(
    nn.Conv2d(3,channel_1,kernel_size=3,padding=1),
    nn.ReLU(),
    nn.Dropout2d(p=0.25),
    nn.MaxPool2d(kernel_size=2),
    nn.BatchNorm2d(channel_1),
    nn.Conv2d(channel_1,channel_2,kernel_size=3,padding=1),
    nn.ReLU(),
    nn.Dropout2d(p=0.25),
    nn.MaxPool2d(kernel_size=2),
    nn.BatchNorm2d(channel_2),
    nn.Conv2d(channel_2,channel_3,kernel_size=3,padding=1),
    nn.ReLU(),
    nn.Dropout2d(p=0.25),
    nn.MaxPool2d(kernel_size=2),
    Flatten(),
    nn.Linear(channel_3*4*4,1024),
)
optimizer=optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9,nesterov=True)


# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
################################################################################
#                                 END OF YOUR CODE                             #
################################################################################

train_part34(model, optimizer, epochs=10)
```
**自此 assignment2圆满结束**

assignment3!!
===

不知不觉就完成了assignment3 但是看起来是顺序下来的三个as 其实assignment3的难度相比前两个要高不少（主要是在逻辑上）

# Rnn
首先开头便是循环神经网络 RNN
RNN是一种用于处理序列数据的神经网络，其核心思想是引入时间维度，让网络能够记住之前的信息，并将其用于当前的计算。

用人话来说 就是rnn相对于fcn这类一般的神经网络多了一个记忆功能 抽象的角度来说 就是每一次要得到ht 会输入x以及上一次的ht-1

类似于数学中的递推

用公式来看 就是

**h=σ（Wx*x+Wh*h(t-1)+b）**

还是很好理解的

从代码的实现来看 主要就是实现prev_h和next_h的迭代 
 在作业中 要求的内容是 为图像添加字幕 那么又涉及到对单词的转换 

 这里的做法是 使用词嵌入技术（word_embedding_forward）将单词转化为低维的密度向量

 也就是和之前将图像转化为矩阵类似 

 然后经过循环神经网络-全连接神经网络 最终选出得分最高的单词 并将得到的单词作为下一格单词预测的输入值

 值得一提的是 由于标题的长度不同 在实现词嵌入的时候 会使用NULL进行补全 从而在之后softmax计算损失值时 会通过加入mask来判断哪些需要并入计算（也就是舍弃NULL）

 #  LSTM 
 由于RNN在循环中会进行大量的矩阵乘法 从而导致梯度爆炸 为了解决这种为题 提出了升级版的RNN  也就是LSTM

 在整体逻辑上 相比于RNN 从表面上看只是多了一个 **c**  

 但实际上 LSTM加入了 遗忘门（Forget Gate）、输入门（Input Gate）和输出门（Output Gate）。这些门的作用是控制信息的流动，从而实现对长短期记忆的管理。

## LSTM 核心公式

### 遗忘门（Forget Gate）
$$
\[ f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \]
$$

### 输入门（Input Gate）
$$
\[( i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \]
$$




### 单元状态更新（Cell State Update）
$$
\[ C_t = f_t * C_{t-1} + i_t * \tilde{C}_t \]
$$

### 输出门（Output Gate）
$$
\[ o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \]
$$
### 隐藏状态（Hidden State）
$$
\[ h_t = o_t * \tanh(C_t) \]
$$

解释一下 首先在遗忘门 通过激活函数sigomd将pre值分为1（记住）和0（遗忘）的俩部分 

然后输入门时 使用sigond对输入的值哪些需要更新进行分类 然后通过tanh创建一个新的候选值向量  
C
，这些候选值将被加入到单元状态中。

然后单元状态更新 完成了信息的过滤和更新 

然后最后的输出门 就是先将正经的output输出 然后通过tanh（c）的单元状态结合 从而获得隐藏状态（舍弃掉旧的 放入信的）
、
便是这样一个过程

代码的实现中就比较简单了 这里的公式看似有多种权重 但是在代码的实现中 其实是共享权重的 因此代码非常简单 只需要将mm得到的x分成四份即可


# GAN（生成对抗网络）

我个人认为算是比较简单的神经网络了（相对于后面两种）

不同于先前学习的学习预测型神经网络 GAN是一种生成网络 通过训练生成出以假乱真的模型的网络 

GAN 的核心是一个生成器和一个鉴别器

生成器和鉴别器都分别是一个深度神经网络（一般就是卷积和全连接）

然后交替训练 
过程大概是
1.先从训练集中挑选一批真实图像
2.给生成器输入一段随机噪音 让他生成假图像
3.让鉴别器学习鉴别出真假图像 真为1 假为0
4.再训练生成器 让他生成出能够让鉴别器给出1的假图像 此时鉴别器权重不变
5。在训练鉴别器 让他能给假图像给出假的评分
6.以此类推....

这就是对抗的过程 生成器目的始终是生成以假乱真 鉴别器目的是明辨是非 在互相竞争中进步 学习 最后能得到真的假图片

代码的实现中 
由于多使用pytorch 实现很简单 loss的求值是使用**二元交叉熵**

# transformer
最难理解的一个 反正我看了好多个视频才勉强理解

transformer实现的功能跟rnn类似 但是实现的方式大相径庭

transformer引入了一个概念 self-attention（自注意）

由**编码器**和**解码器**组成
编码器工作
首先 由于没有时间序列的概念 这里引入了位置编码的方式 通过对输入的字给予相应的位置 从而使其有序

然后就是多头注意力机制

他的核心在于三个向量 
**query**

**key**

**value**
对于输入的每个句子 都会分成这三个部分

query:表示当前需要关注的内容。
Key（键）：表示其他位置的内容，用于与 Query 进行匹配。
Value（值）：表示每个位置的实际内容，用于生成最终的输出。

然后会经过一个线性变换 

首先 对于每一个query 会和每一个key求相似度 得到注意力分数 **注意**这里会除以一个sqrt（dk）防止梯度爆炸

然后对于每一个query的注意力分数 通过softmax（）求概率分布  然后依据这个分布 对value进行加权和 从而得到ouput

然后ouput会经过一个前馈神经网络（一个线性层 一个激活层）
最后通过归一化等数据处理 就能得到结果

而编码器得到的结果是一个隐藏状态 那么解码器的任务就是通过这个信息得到预测的序列

解码器工作

首先既然是预测 会防止偷看答案 因此需要掩码 然后同上 进行多头注意力操作

其他操作如出一辙 最后得到结果

## Transformer 的优势
并行化能力强：由于自注意力机制不依赖于序列的时间顺序，Transformer 可以并行处理序列中的所有元素，大大提高了训练和推理速度。

能够捕捉长距离依赖：通过自注意力机制，Transformer 能够有效地捕捉序列中的长距离依赖关系。

灵活性高：Transformer 架构可以应用于多种任务，如机器翻译、文本生成、文本分类等。

代码的实现虽然很难 但是由于使用pytorch 很多函数并不需要自己写 因此难度还好 主要难在理解上面

# 自监督学习
个人认为理解第二难 现在讲讲

首先是作用 我们知道 在训练中 通常需要认为的给图像给予标签 再用于训练和预测 因此自监督就是而是利用数据本身的结构和关系来自动生成标签

原理 总的来说 分为
掩码语言模型（Masked Language Modeling, MLM）：在自然语言处理中，随机掩盖句子中的某些词汇，让模型预测被掩盖的词汇。

图像自编码器（Image Autoencoders）：在图像处理中，模型学习重建被部分遮挡的图像。

对比学习（Contrastive Learning）：通过生成相似与不相似的样本对，训练模型学习如何将相似样本拉近，将不相似样本拉远。

而在作业中 要求实现的是SimCLR方法

SimCLR 的核心思想是通过最大化同一图像的不同数据增强版本之间的相似性
原理分为

数据增强：对同一张图像进行两种不同的随机数据增强（如随机裁剪、颜色失真、高斯模糊等），生成正样本对。

特征提取：使用深度神经网络（如 ResNet 或 Vision Transformer）提取图像的特征表示。

特征投影：通过一个小的全连接神经网络（MLP）将特征投影到一个低维空间。

对比损失：计算正样本对之间的相似性，并通过 InfoNCE 损失函数优化模型，使得正样本对的特征表示更接近，负样本对的特征表示更远离。

代码的实现也很清晰 只需要跟着公式一步步来即可 难在理解上、

==========================================================================================================================
    **到此 cs231n圆满结束！**



