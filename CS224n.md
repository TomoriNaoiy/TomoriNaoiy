CS224n
=========
学完了cs231n 继续深造nlp

# one-hot、
独热编码 一种词向量获得方式 由0和1组成 但是具有过于离散并且词语关联性不强的缺点

# word2Vec
相较于前者的升级版 使用了中心词和上下文词的方式 由中心词预测上下文 并提出了预测率 个人认为和神经网络的评分函数类似 

同样的 既然要优化 需要损失函数
<img width="551" height="140" alt="image" src="https://github.com/user-attachments/assets/d53fd732-a33a-4744-b22b-f6fb60b307cc" />
那么求导自然必不可少
<img width="843" height="264" alt="image" src="https://github.com/user-attachments/assets/9e6f3a4a-ded9-4941-8f24-685e7bf1723e" />

其实这就是skip-gram的核心算法

这种词向量的获取方式 对于nlp是非常实用且重要的
<img width="1103" height="693" alt="image" src="https://github.com/user-attachments/assets/d4e8973f-f88f-4934-8c73-5a08d8e51c58" />
如此可以看到 获得的词语能够以类相聚







