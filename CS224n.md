CS224n
=========
学完了cs231n 继续深造nlp
# L1
## one-hot
独热编码 一种词向量获得方式 由0和1组成 但是具有过于离散并且词语关联性不强的缺点

## word2Vec
相较于前者的升级版 使用了中心词和上下文词的方式 由中心词预测上下文 并提出了预测率 个人认为和神经网络的评分函数类似 

同样的 既然要优化 需要损失函数
<img width="551" height="140" alt="image" src="https://github.com/user-attachments/assets/d53fd732-a33a-4744-b22b-f6fb60b307cc" />
那么求导自然必不可少
<img width="843" height="264" alt="image" src="https://github.com/user-attachments/assets/9e6f3a4a-ded9-4941-8f24-685e7bf1723e" />

其实这就是skip-gram的核心算法

这种词向量的获取方式 对于nlp是非常实用且重要的
<img width="1103" height="693" alt="image" src="https://github.com/user-attachments/assets/d4e8973f-f88f-4934-8c73-5a08d8e51c58" />
如此可以看到 获得的词语能够以类相聚

更神奇的是 词向量之间甚至可以线性计算
v(king)-v(man)+v(women)得到的词向量，最相似的就是v(queen)！
# L2
上文提到skip-gram算法

那么什么是sg算法呢

其实就是在word2Vec的基础上 增加了一个窗口（window） 然后通过这个窗口大小来筛选上下文词

这篇讲义主要说了skip-gram算法的组成部分

## 对中心词的损失函数
<img width="997" height="160" alt="image" src="https://github.com/user-attachments/assets/e9fa7487-0424-4bf8-887b-3e65bb98a00f" />
由于common方法会导致求导计算量大幅升高 我们使用负采样收集的方法
<img width="1149" height="1043" alt="image" src="https://github.com/user-attachments/assets/7f37a649-c29a-43d4-85a5-8aed55a5c846" />

# 对滑动窗口的损失函数
<img width="1013" height="162" alt="image" src="https://github.com/user-attachments/assets/66320a49-2426-4f19-9641-09038200a4b4" />

总结来说 skip-gram的算法的训练结果是给出一个中心值 能够推测出上下文词 比如给出sit单词 能够预测到on 或者in等单词

# as1
完成对word2vec的逻辑以及对于相似度的认识

没什么好讲的 主要是```similar_words = wv_from_bin.most_similar(word, topn=10)```这个函数来获得相似单词

以及```pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'entertainment'], negative=['man']))```

通过类比获得单词


