# 调参
顾名思义 调参就是对我们的模型进行参数的调整 以便其适应数据 达到更好的准确率

在讲调参前 我们得先知道 为什么要调参 怎么调参

### 为什么？
我们要知道 对数据进行一整轮的评估预测 大概有以下几个流程

1. **观察数据和要求**：  
首先你要知道 给予的数据很多其实是比较差的 我们称为’脏数据‘ 如果你打算得到一个很平庸的结果 那么你大可以跳过这一步和下一步直接硬套数据 而且很重要的 你需要在观察后发现文字的长度（或者图片）决定是否padding 是否有空值（这决定了您之后是否需要进行判断或者补充） 标签的平衡性（1或者0的数量）

2. **清洗数据** ：
这部可能需要分情况 有时需要‘上下文’这时候清洗后数据反而导致得分下降 总之必要的就是去掉html标签 乱码 拼写错误等等

3. **特征选择**：
也许你会疑惑 **深度学习**和**机器学习**最大的区别不就是深度学习能够自己选择特征吗？  
*那么问题来了 你觉得让一个人期末周复习后去考试效果好还是直接给他答案去考试效果好呢？）*

4. **数据增强**：
假如你的数据不够多 那么我们通常会采用英 -> 中 -> 英的策略 使得标签不变 但是词可能变了 比如 把 "disaster" 换成 "calamity" 这样我们就获得了更多的数据

5. **模型选择**：最重要的一步
你可能需要找到你认为你合适的模型 然后发现他不那么合适 然后把模型拿出来 进行魔改 偷偷混入dropout或者加一层LSTM 

6. **调参** 
绕了一大圈 终于回来了 在你确定模型之后 的确就应该开始对调参进行斟酌了 调参就是在已有模型的基础上 进一步提升对数据的适应 虽然提升可能不如前者大 但确实很吃操作的一步 但是这一块先不细说 待会仔细分析

7. **最终集成**
有时候可能不一定有最优的那个模型 这时候我们可以用多个模型 然后去加权平均

---

## 以上
---

那么讲完了步骤 你应该也了解 为什么要调参了

## 怎么调参？
首先要清楚 参数分为三种：
1. **网络参数**：神经网络的层间参数 如卷积核尺寸 激活函数 网络层数 等等
2. **优化参数**：学习率 批量大小（batch） 优化器的参数等等
3. **正则化参数**： 如权重衰减 dropout率等等

# 那我们开始 
## 学习率(l_r)
最**重要**的一个超参数  
你可以理解为浅略的理解成步长(根据loss修改权重的步长大小)   
**过大**的学习率可能导致跳过最优权重 **过小**的学习率可能导致收敛过慢或者困在**局部**最优点出不来

### 这里给几个参考范围

- SGD [1e-2,1e-1]
- momentum [1e-3,1e-2]
- Adam [1e-3,1e-2]

## 批次大小(batch_size)
你可以理解为 模型看多大的size 对权重进行一次更新 越小说明更新的越频繁

**如果过小:[8,32]**  
模型可能因为一次小批量且完全错误的批数据导致权重被带偏   
**优点**是 可能走出局部死路 提供更高的**泛化性**

**如果过大:[64,256]**  
模型的更新会更加平滑且稳定 但是有可能收敛到尖锐的**极小值** **泛化性更低**

---

### 一条法则 batchsize和l_r一般同时增加或减少

## 轮数(epoches) ---- 火候[3,5]
你可以理解为 将数据**重复**学习几遍

**很好理解 如果过小[1,2] 训练效果较差**
**如果过大[5,10] 可能过拟合**


## 权重衰减参数
属于正则化的参数 可以理解为 如果某个权重非常大 对其进行惩罚(如果过拟合 可能会有极大的参数强行记忆答案) 使得权重进行贴近

**对于数据少的情况 过拟合的概率升高 此时提高权重衰减参数[0.01,0.1]**

## 标签平滑参数
将预测标签的值进行归一化 也就是原本是1和0的区别 我么将其改为0.9和0.1的区别

**作用就是 如果数据"脏" 可以出现错误的划分 如果强行用1,0进行模拟 可能会导致模型强行改变权重 对结果有较大影响**

## dropout参数(随机失活)
了解过神经网络的应该都清楚这个模块 就是对数据进行随机选择 增强鲁棒性和泛化性  
**一般0.1**

---

还有一些比较高级的参数和技巧

## 预热参数(warmup)
由于参数可能有较大的变化(或者从零开始训练) 如果一开始就以最大的速度进行梯度改变 可能直接把权重的方向弄乱 因此刚开始让学习率降低 然后逐渐升高

## 梯度累加
**时间换空间** 也就是如果你的显存不足以支持你用64的batch_size 那么你可以先进行累加 用4的batch_size 然后每累积16次进行一次更新

## 余弦退火
让学习率以余弦函数类似的曲线进行平滑的减少 这样使得能找到更精细的平衡点

## 冻结梯度
**跟warm_up**有些类似 但是这里是
```param.requires_grad = False```
先把权重变化锁住 但是分类器会进行学习

## k-fold(交叉训练)
**非常常用的技巧 由于一开始是20%测试80%学习 总有20%学不到**  
那么我们使用对5份20%分别做为测试集 就能测试所有的样本

