由于这部分机器学习的确难度不小 个人感觉完成assignment1之后受益匪浅（其实也就是大概明白了部分实现和基本原理） 因此写一些笔记记录一下学到的部分细节和重点 以便回顾

Assignment1
===
# K-nn
这部分比较简单 几乎算不上分类器 也就是单纯的对特征逐个比较 然后寻找”距离“最小的图片。这样的效率很低 准确率也很低 总体来说思路并不算难 重点就是代码的实现
**代码实现** 主要难点就是矢量化的实现 多使用广播机制
# SVM
要讲svm 就先讲讲线性分类器 其中包含了评分函数（score function）和损失函数（loss function）

评分函数 即是通过f（w，x，b）=Wx+b的函数对一个图片评分 然后分类
而 w是权重 类似于一个类别对于图像的各个特征的重视程度
而不同的w对于分类图像可能会有不同的效果 因此我们需要对这个评分的结果进行评价

也就是 损失函数

而 svm 便是使用其中一种损失函数进行分类的 多类支持向量机（SVM）损失函数

那么svm的损失函数如何计算？

$$
**L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)**
$$

而在实际操作中 具体函数便是

$$
L_i = \sum \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)
$$

也就是说 如果预测与实际的差高于某个值 我们便对其进行惩罚

我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。

### 正则化
上面损失函数有一个问题。假设有一个数据集和一个权重集W能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有
）。问题在于这个W并不唯一：可能有很多相似的W都能正确地分类所有的数据。一个简单的例子：如果W能够正确分类所有数据，即对于每个数据，损失值都是0。那么当
时，任何数乘
都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对W乘以2将使得差距变成30。

换句话说，我们希望能向某些特定的权重W添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个正则化惩罚（regularization penalty）
部分。最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：

$$
R(W) = \sum_{k} \sum_{l} W_{k,l}^2
$$

也就是说 最后的公式是

$$
L = \frac{1}{N} \sum_{i} L_i + \lambda R(W)
$$

### 最优化
这便是找到最优w的过程

通过公式可见，每个样本的数据损失值是以
为参数的线性函数的总和（零阈值来源于
函数）。有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。

而我们如何找到最优w呢 如果采用随机w 并不断取优 的确有一点效果 但效率极低且浪费算力

于是我们使用**随机梯度下降**
说到梯度 简单来说 就是损失函数的斜率

既然我们的目的是减小损失值（越小说明模型效果越好或者训练效果显著） 那么我们只需要根据负梯度更新参数 便可以最高效的找到w

而svm中 我们只需要对损失函数微分 就能得到梯度公式

$$
\nabla_{w_{y_{i}}} L_{i} = -\left( \sum_{j \neq y_{i}} 1(w_{j}^{T} x_{i} - w_{y_{i}}^{T} x_{i} + \Delta > 0) \right) x_{i}
$$

大概解释一下 就是非正确分类的部分 大于0就是1 小于0就是0

正确分类的部分 减去非零部分的个数和

经过这些操作 就能获得梯度 然后只需要在每次训练时向着负梯度的方向下降 就能逐渐优化W 从而减小损失值

**代码实现**在svm的py作业中 难点依旧在矢量化的实现 在显性循环的部分 只需要分行计算得分 求出margin 讨论后在分别求出损失值和dW

而在矢量化实现中 我们需要使用 **gradient[np.arange(num_train),y]=-row_num**方法区分出正确分类的部分 并使用矩阵乘法完成加减的实现

当然最麻烦的部分还是对与广播机制的使用以及对矩阵维度和大小的reshape（容易错 多看ai 反正我是这样）

# softmax
这个其实跟svm殊途同归 个人认为都近似属于单层神经网络 但softmax的命名并非是损失值的计算 而是来自于其对于评分函数的处理

**softmax**会对得到的评分进行归一化处理（我也不知道是不是这么说） 个人理解呢 就是将其转化为相对概率

比如svm得到的评分可能是[100,-1,114514]而softmax对评分进行函数处理后会变成类似于[0.1,0.0001,0.8999]

而softmax处理后 得到的dscore就会用于损失值的计算

（当然softmax还有一个特点就是需要减去np.max（f）,防止数值爆炸）

那么就是softmax的梯度了

这里的梯度就不能单纯微分得到了 而是需要求偏导了
也就是最简单的反向传播（其实就是倒推） 从得到的dscore倒推得到dW
笔记中没有给出公式 当然我也不会求偏导 于是询问ai

$$
\nabla_{\mathbf{W}} \mathcal{L}_i = -\mathbf{X}_i (\text{softmax}(\mathbf{z}_i) - \mathbf{y}_i)
$$

用人话来说 就是正确分类的部分 x[i]*(p-1) 非正确分类的部分x[i]*p

**（思路都挺简单 但我在这里矢量化的过程卡了大半天........）**

最后就是记得取平均和正则化了
# 双层神经网络
**重头戏来了**
经历了前俩的铺垫 最后难点来了 两层神经网络
其实也就是三层 input层 hidden层 以及output层

首先input输入一个数 然后进行一次线性变化 也就是Wx+B

进入hidden层

然后就是经过非线性变化
**也就是激活函数**
常见的是sigmoid函数 也就是1/1+e{-x}

但是由于其缺陷 例如梯度死亡的问题 所以我们使用  ReLU函数

这个函数炒鸡简单 就是max（0，x）

也就是只要小于0 就归0 大于0保留

那么经过激活函数后 再经过一次线性变换就是最终得分了

这个过程 从前向后推 自然就是向前传播的过程 

而损失值的计算 既然也属于这一类

但这里的损失值有一些区别 我们通过最终得分得到的损失值需要经过W1以及W2的正则化

**梯度**
这部分应该可以算是整个作业的最难点了

要求这里的梯度 很明显我们要使用反向传播 通过链式法则求偏导 一步步得到dW1以及dW2

$$
d\mathbf{W}_1 = \frac{\partial \mathcal{L}}{\partial \mathbf{W}_1} = \delta_2 \mathbf{a}_1^T
$$

$$
d\mathbf{W}_2 = \frac{\partial \mathcal{L}}{\partial \mathbf{W}_2} = \delta_3 \mathbf{a}_2^T
$$

用人话来说 就是通过偏导公式先得到dW1(正则化)

然后需要获得hidden层的dscore 然后反向通过激活函数 最后以hidden层的dscore推倒出dW1

**注意**这里的每一步都需要正则化（激活函数部分除外）

最后将梯度存入字典即可

这样以来 神经网络的大体框架就出来了

# 最后 调整超参数 
一个几乎在每个任务中都会出现的部分 实现并不难 也就是通过循环 使用不同的参数来调用solver 最终得到不同的效果

以上是完成assignment1以及部分note后的反思

Assignment2
===
今天是2月8号 终于完成了assignment2！可喜可贺 

那就趁热打铁 谈谈assignment的学习收获

# 全连接神经网络
和as1的两层神经网络的区别就是 **更多层了（**

当然没有这么浅显 全连接神经网络主打的是 **任意层**
也就是说 可以有无数层

而代码的实现中 也就是用上了循环 以循环的方式初始化W和b 唯一要注意的就是输出层的特殊性

而在loss的求值代码实现中 唯一的重点就是通过循环从而将w与系数一一对应起来 至于score 和激活函数什么的 我们早在as1就完成了函数 这里只需要调用即可

而反向传播的过程与上面不尽相同 要注意的点就是值的迭代了 （有些麻烦 卡了挺久） 

## 然后就提到了rms和adam

这两中方法实际就是对学习率的适应 根本上来说也就是提升效率的方法 

实现上也很简单 只需要跟着公式来即可 

全部完成了之后 又到了喜闻乐见的参数调优环节  经过几轮的洗礼 学习率的基本最优解便是1e-3倒5e-4了

# 批量归一化 

批量归一化是一种有效的技术，用于加速深度神经网络的训练，提高模型的稳定性和性能。它通过归一化输入数据，减少内部协变量偏移，使得每一层的输入分布更加稳定。

人话来说 就是对于数据的预处理 从而提高效率

$$
d\mu = \frac{1}{N} \sum_{i=1}^{N} x_i
$$

$$
d\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2
$$

### 3. **归一化**
$$
d\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$

### 4. **缩放和偏移**
$$
d\y_i = \gamma \hat{x}_i + \beta
$$

重要的点就是要记得更新running_mean便是

反向传播公式就比较复杂了 但跟着公式一步步解决即可

然后又出现一个新概念
# Layer Normalization

实际上便是批量归一化的一种变体 批量归一化相当于是对列求平均和方差 而ln则是对行求平均和方差 相对于批量归一化 由于特征值数量大体固定 因而ln受到输入值数量的影响更小 在部分情况下更加适用

然后便是

# 随机失活（dropout）

随机失活（Dropout）是一种用于深度学习模型的正则化技术，旨在减少模型的过拟合现象。其核心思想是在训练过程中，随机地将网络中的一些神经元的输出置为零，从而减少神经元之间的依赖关系，提高模型的泛化能力。

人话来说 就是每个数据又（1-p）的概率被ban掉 换种说法就是 从x个里面只使用px个数据点 这样可以防止大量的训练数据使得其过拟合 提高泛化能力

代码的实现就非常简单了 唯一要注意的点就是结果要÷p（相当于期望从x变成了px 结果要再变回x就要除以p）

## 重中之重
# 卷积神经网络~
刚入学就听说了 如今终于是接触到了 ~~但好像也没那么牛逼~~

首先从直观上来讲 cnn比于fcn 多了卷积层和池化层

### 卷积层 
首先从功能角度来说 卷积层相当于设置了一个特征野（我个人习惯教程特性检查器） 通过这个特性检查器 对图像特征进行局部提取 
例如 

1 0 0 0 1

0 1 0 1 0

0 0 1 0 0

0 1 0 1 0

1 0 0 0 1

那么特征野假设是一个3*3的大小的

1 2 3

4 5 6

7 8 9

那么就对

1 0 0

0 1 0

0 0 1

进行点乘 然后放到一个矩阵里面 然后移倒下一格

000

101

010

然后一个个移动下去 由于有时会导致数据丢失 所以经常会对边缘进行0填充 这样就完成了卷积的过程 

代码的实现上
由于在作业中 要求的是用最直观的方式 那便使用显示循环 通过矩阵切片的方式点乘 从而完成卷积

反向传播也大体相似 只是需要将dx的填充部分切片去除掉

个人理解上 相对于线性分类器 卷积层不同于从单纯的将特征进行线性转化 而是在空间上以一个具有大小的特征检查器对特征进行一块块的提取 这样明显具有更高的效率
## 然后是最大池化
在效果上 实际就是对数据的浓缩

例如64*64大小的数据 通过2*2的最大池化后 变成了32*32

而其中提取最大值的过程 筛选掉了无用的数据 而减小了特征的大小 也提高了训练的速度

代码实现上和卷积层基本相似 只是变成取最大值的过程 以及引入了pool_height

## 然后空间归一化 和前面的批归一化同根同源 只是面对多维的数据 将特征通道移至末尾 以便于展平和求均值

# Pytorch
在这个作业中 大致讲述了pytorch的使用过程

首先是数据集的加载
```python
NUM_TRAIN = 49000
transform = T.Compose([
                T.ToTensor(),
                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
            ])

cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,
                             transform=transform)
loader_train = DataLoader(cifar10_train, batch_size=64,
                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))

cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,
                           transform=transform)
loader_val = DataLoader(cifar10_val, batch_size=64,
                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))

cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True,
                            transform=transform)
loader_test = DataLoader(cifar10_test, batch_size=64)
```
然后便是一些API
首先要定义一个展平函数 
```python
def flatten(x):
    N = x.shape[0] # read in N, C, H, W
    return x.view(N, -1)  # "flatten" the C * H * W values into a single vector per image
```
然后导入包
```
import torch.nn.functional as F
```
这是一个简单的model 包含一些卷积 激活 池化等函数 
可以通过
F.ReLU()

x=F.conv2d(x,conv_w1,conv_b1,padding=2)

等方式调用

中间的barebone就不多说了 直接讲model API
```python
class ThreeLayerConvNet(nn.Module):
    def __init__(self, in_channel, channel_1, channel_2, num_classes):
        super().__init__()
        ########################################################################
        # TODO: Set up the layers you need for a three-layer ConvNet with the  #
        # architecture defined above.                                          #
        ########################################################################
        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
        self.conv1=nn.Conv2d(in_channel,channel_1,kernel_size=5,padding=2)
        self.conv2=nn.Conv2d(channel_1,channel_2,kernel_size=3,padding=1)
        self.fc3=nn.Linear(channel_2*32*32,num_classes)
        nn.init.kaiming_normal_(self.conv1.weight)
        nn.init.kaiming_normal_(self.conv2.weight)
        nn.init.kaiming_normal_(self.fc3.weight)

        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
        ########################################################################
        #                          END OF YOUR CODE                            #
        ########################################################################

    def forward(self, x):
        scores = None
        ########################################################################
        # TODO: Implement the forward function for a 3-layer ConvNet. you      #
        # should use the layers you defined in __init__ and specify the        #
        # connectivity of those layers in forward()                            #
        ########################################################################
        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
        x=F.relu(self.conv1(x))
        x=F.relu(self.conv2(x))
        x=flatten(x)
        scores=self.fc3(x)


        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
        ########################################################################
        #                             END OF YOUR CODE                         #
        ########################################################################
        return scores


def test_ThreeLayerConvNet():
    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size [3, 32, 32]
    model = ThreeLayerConvNet(in_channel=3, channel_1=12, channel_2=8, num_classes=10)
    scores = model(x)
    print(scores.size())  # you should see [64, 10]
test_ThreeLayerConvNet()
```
先构建神经网络的类 
然后
```python
hidden_layer_size = 4000
learning_rate = 1e-2
model = TwoLayerFC(3 * 32 * 32, hidden_layer_size, 10)
optimizer = optim.SGD(model.parameters(), lr=learning_rate)

train_part34(model, optimizer)
```
便可以直接使用

# PyTorch Sequential API（最好用的一个）
```python
class Flatten(nn.Module):
    def forward(self, x):
        return flatten(x)

hidden_layer_size = 4000
learning_rate = 1e-2

model = nn.Sequential(
    Flatten(),
    nn.Linear(3 * 32 * 32, hidden_layer_size),
    nn.ReLU(),
    nn.Linear(hidden_layer_size, 10),
)

# you can use Nesterov momentum in optim.SGD
optimizer = optim.SGD(model.parameters(), lr=learning_rate,
                     momentum=0.9, nesterov=True)

train_part34(model, optimizer)

```
这是大致的实现过程

**下面是使用pytorch完成的一个目前最佳的神经网络 至于更多的细节之后再单独写一个来讲 这里直接挂出来吧**
```python


################################################################################
model = None
optimizer = None

# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
channel_1 = 32
channel_2 = 128
channel_3 = 256
learning_rate=1e-3
model=nn.Sequential(
    nn.Conv2d(3,channel_1,kernel_size=3,padding=1),
    nn.ReLU(),
    nn.Dropout2d(p=0.25),
    nn.MaxPool2d(kernel_size=2),
    nn.BatchNorm2d(channel_1),
    nn.Conv2d(channel_1,channel_2,kernel_size=3,padding=1),
    nn.ReLU(),
    nn.Dropout2d(p=0.25),
    nn.MaxPool2d(kernel_size=2),
    nn.BatchNorm2d(channel_2),
    nn.Conv2d(channel_2,channel_3,kernel_size=3,padding=1),
    nn.ReLU(),
    nn.Dropout2d(p=0.25),
    nn.MaxPool2d(kernel_size=2),
    Flatten(),
    nn.Linear(channel_3*4*4,1024),
)
optimizer=optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9,nesterov=True)


# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
################################################################################
#                                 END OF YOUR CODE                             #
################################################################################

train_part34(model, optimizer, epochs=10)
```
**自此 assignment2圆满结束**



