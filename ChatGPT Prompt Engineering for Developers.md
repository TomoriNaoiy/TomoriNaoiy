# ChatGPT Prompt Engineering for Developers'
由吴恩达及OpenAi工作人员共同开的课 主要围绕提示词工程展开

# llm sort
1. Base llm
我们更多的称其为预训练模型 其实就进过大量数据训练后 能够根据上下文 对后面的单词进行预测  
比如 我们询问 森林里面有一群蓝精灵  
模型就会生成“还有一群红精灵 他们愉快的生活在一起”

2. Instruction Tuned LLM
市面上见的更多的模型 主要通过同样的自监督学习 然后能够对问题进行回答

比如 我们询问：“中国的首都”
如果是BL可能生成“中国的美食”“中国的政治中心”等等
ITL就是生成“中国的首都在北京”

---

总而言之 我们的提示词工程肯定是聚焦在后者的 由于提示词的不明确或者模糊都可能导致实际的回答效果并不好

# 正题开始
1. 首先第一个原则：**指令的清晰明确和具体**
2. 原则二： 给予llm一定时间去思考

先说第一个原则：其中依旧有四个点
1. 在prompt中给予所需文本 使用''''''或者<>等（python当然用})
2. 结果形式标准化 如返回html JSON结构
3. 检查文本是否复合要求 如是否是step1,2,3排序等等
4. 简短例子 给予ai现成的例子能够使其完成任务的效果更好


接下来说第二个原则：课程中的说法可能有些怪 实际上解释就是 给予ai更明确的步骤
1. 给予明确步骤：如果你写了一大串要求 ai可能会漏掉或者模糊 这时我们需要写成1. 做什么 2. 做什么 
2. 得出结论时 先用自己的方法做一遍
**我认为这是很重要的点** 我们经常把一个结论抛给ai 然后得到他的判断 此时的ai很可能是直接对结论简要的看了一眼 觉得没问题就判断是对的 但是实际上步骤很可能都有问题 因此

我们需要让ai先自己做一遍 然后再来对照用户给的情况

3. 减少幻觉
我们通用的ai（尤其是豆包） 常常会给出根据数据给出一些一本正经但是根本不纯在的答案 我们称为**幻觉**

如何减少幻觉呢 我们需要在提示词中告诉他 请找到相关文献 并根据文献来回答问题 这样得到的幻觉就会减少

# 提示词的迭代工程
我觉得说的很好的一句话
*没有普适的所谓完美提示词，更好的办法是给出一个问题再在此基础上不断迭代和优化提示词最终得到合适的结果。*

课程中也是这么做的

可以从最初的版本开始 然后一次次运行 然后关注句子长度 面对对象 语言模式等等方面 对prompt进行迭代更新 来做到优化你的提示词

# temperaure
温度参数 在api中默认为0

课程中提到 例如在写信的时候 我们也许希望写的信并不是唯一的 也就是更具有多样性 那我们需要调整这个参数

举个例子 假如问出 我最喜欢的食物
```
面包0.7 西瓜0.1 菠萝0.2
```
如果是0.0 的temperature 那么一定结果就是 我最喜欢吃面包 

但是如果是0.3的temperature 那么可能就会选到菠萝

如果0.7 那么西瓜也有可能出现 

也就说 温度的参数使得结果更加具有多样性

# 三个role的分工
做为一个聊天机器人 我们呢需要有三个role 而不是单纯的一个提示词给出任务了
1. assistant
2. system
3. user
先说user 就是我们提出问题的角色 也就是俗称的输入端

其次是assistant 这里可能有些反直觉 但是其实助手是回答的角色 也就是**输出端** 其实 assistant就是AI本身

最后是system system其实就是“**人设**” 你给他的应该是 你是一个....人 你的职责是... 你需要用什么语气...等等

除了人设 system还有一个很重要的作用 就是把对话发给下一轮回答的助手 维持**上下文**

**大模型本身是没有记忆的 是每一轮问答把上一轮的问答放入user 才使得大模型具有了记忆 这一部是system做的**


