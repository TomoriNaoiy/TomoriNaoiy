由于这部分机器学习的确难度不小 因此在这里做一个笔记 以便之后回顾
# K-nn
这部分比较简单 只需要了解其中的算法即可 便不讲了
# SVM
要讲svm 就先讲讲线性分类器 其中包含了评分函数（score function）和损失函数（loss function）

评分函数 即是通过f（w，x，b）=Wx+b的函数对一个图片评分 然后分类
而 w是权重 类似于一个类别对于图像的各个特征的重视程度
而不同的w对于分类图像可能会有不同的效果 因此我们需要对这个评分的结果进行评价

也就是 损失函数

而 svm 便是使用其中一种损失函数进行分类的 多类支持向量机（SVM）损失函数

那么svm的损失函数如何计算？

$$
**L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)**
$$

而在实际操作中 具体函数便是

$$
L_i = \sum \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)
$$

也就是说 如果预测与实际的差高于某个值 我们便对其进行惩罚

我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。

### 正则化
上面损失函数有一个问题。假设有一个数据集和一个权重集W能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有
）。问题在于这个W并不唯一：可能有很多相似的W都能正确地分类所有的数据。一个简单的例子：如果W能够正确分类所有数据，即对于每个数据，损失值都是0。那么当
时，任何数乘
都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对W乘以2将使得差距变成30。

换句话说，我们希望能向某些特定的权重W添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个正则化惩罚（regularization penalty）
部分。最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：

$$
R(W) = \sum_{k} \sum_{l} W_{k,l}^2
$$

也就是说 最后的公式是

$$
L = \frac{1}{N} \sum_{i} L_i + \lambda R(W)
$$

### 最优化
这便是找到最优w的过程

通过公式可见，每个样本的数据损失值是以
为参数的线性函数的总和（零阈值来源于
函数）。有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。

而我们如何找到最优w呢 如果采用随机w 并不断取优 的确有一点效果 但效率极低且浪费算力

于是我们使用**随机梯度下降**
说到梯度 简单来说 就是损失函数的斜率

既然我们的目的是减小损失值（越小说明模型效果越好或者训练效果显著） 那么我们只需要根据负梯度更新参数 便可以最高效的找到w











