由于这部分机器学习的确难度不小 个人感觉完成assignment1之后受益匪浅（其实也就是大概明白了部分实现和基本原理） 因此写一些笔记记录一下学到的部分细节和重点 以便回顾
# K-nn
这部分比较简单 几乎算不上分类器 也就是单纯的对特征逐个比较 然后寻找”距离“最小的图片。这样的效率很低 准确率也很低 总体来说思路并不算难 重点就是代码的实现
**代码实现** 主要难点就是矢量化的实现 多使用广播机制
# SVM
要讲svm 就先讲讲线性分类器 其中包含了评分函数（score function）和损失函数（loss function）

评分函数 即是通过f（w，x，b）=Wx+b的函数对一个图片评分 然后分类
而 w是权重 类似于一个类别对于图像的各个特征的重视程度
而不同的w对于分类图像可能会有不同的效果 因此我们需要对这个评分的结果进行评价

也就是 损失函数

而 svm 便是使用其中一种损失函数进行分类的 多类支持向量机（SVM）损失函数

那么svm的损失函数如何计算？

$$
**L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)**
$$

而在实际操作中 具体函数便是

$$
L_i = \sum \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)
$$

也就是说 如果预测与实际的差高于某个值 我们便对其进行惩罚

我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。

### 正则化
上面损失函数有一个问题。假设有一个数据集和一个权重集W能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有
）。问题在于这个W并不唯一：可能有很多相似的W都能正确地分类所有的数据。一个简单的例子：如果W能够正确分类所有数据，即对于每个数据，损失值都是0。那么当
时，任何数乘
都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对W乘以2将使得差距变成30。

换句话说，我们希望能向某些特定的权重W添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个正则化惩罚（regularization penalty）
部分。最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：

$$
R(W) = \sum_{k} \sum_{l} W_{k,l}^2
$$

也就是说 最后的公式是

$$
L = \frac{1}{N} \sum_{i} L_i + \lambda R(W)
$$

### 最优化
这便是找到最优w的过程

通过公式可见，每个样本的数据损失值是以
为参数的线性函数的总和（零阈值来源于
函数）。有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。

而我们如何找到最优w呢 如果采用随机w 并不断取优 的确有一点效果 但效率极低且浪费算力

于是我们使用**随机梯度下降**
说到梯度 简单来说 就是损失函数的斜率

既然我们的目的是减小损失值（越小说明模型效果越好或者训练效果显著） 那么我们只需要根据负梯度更新参数 便可以最高效的找到w

而svm中 我们只需要对损失函数微分 就能得到梯度公式

$$
\nabla_{w_{y_{i}}} L_{i} = -\left( \sum_{j \neq y_{i}} 1(w_{j}^{T} x_{i} - w_{y_{i}}^{T} x_{i} + \Delta > 0) \right) x_{i}
$$

大概解释一下 就是非正确分类的部分 大于0就是1 小于0就是0

正确分类的部分 减去非零部分的个数和

经过这些操作 就能获得梯度 然后只需要在每次训练时向着负梯度的方向下降 就能逐渐优化W 从而减小损失值

**代码实现**在svm的py作业中 难点依旧在矢量化的实现 在显性循环的部分 只需要分行计算得分 求出margin 讨论后在分别求出损失值和dW

而在矢量化实现中 我们需要使用 **gradient[np.arange(num_train),y]=-row_num**方法区分出正确分类的部分 并使用矩阵乘法完成加减的实现

当然最麻烦的部分还是对与广播机制的使用以及对矩阵维度和大小的reshape（容易错 多看ai 反正我是这样）

# softmax
这个其实跟svm殊途同归 个人认为都近似属于单层神经网络 但softmax的命名并非是损失值的计算 而是来自于其对于评分函数的处理

**softmax**会对得到的评分进行归一化处理（我也不知道是不是这么说） 个人理解呢 就是将其转化为相对概率

比如svm得到的评分可能是[100,-1,114514]而softmax对评分进行函数处理后会变成类似于[0.1,0.0001,0.8999]

而softmax处理后 得到的dscore就会用于损失值的计算

（当然softmax还有一个特点就是需要减去np.max（f）,防止数值爆炸）

那么就是softmax的梯度了

这里的梯度就不能单纯微分得到了 而是需要求偏导了
也就是最简单的反向传播（其实就是倒推） 从得到的dscore倒推得到dW
笔记中没有给出公式 当然我也不会求偏导 于是询问ai

$$
\nabla_{\mathbf{W}} \mathcal{L}_i = -\mathbf{X}_i (\text{softmax}(\mathbf{z}_i) - \mathbf{y}_i)
$$

用人话来说 就是正确分类的部分 x[i]*(p-1) 非正确分类的部分x[i]*p

**（思路都挺简单 但我在这里矢量化的过程卡了大半天........）**

最后就是记得取平均和正则化了
# 双层神经网络
**重头戏来了**
经历了前俩的铺垫 最后难点来了 两层神经网络
其实也就是三层 input层 hidden层 以及output层

首先input输入一个数 然后进行一次线性变化 也就是Wx+B

进入hidden层

然后就是经过非线性变化
**也就是激活函数**
常见的是sigmoid函数 也就是1/1+e{-x}

但是由于其缺陷 例如梯度死亡的问题 所以我们使用  ReLU函数

这个函数炒鸡简单 就是max（0，x）

也就是只要小于0 就归0 大于0保留

那么经过激活函数后 再经过一次线性变换就是最终得分了

这个过程 从前向后推 自然就是向前传播的过程 

而损失值的计算 既然也属于这一类

但这里的损失值有一些区别 我们通过最终得分得到的损失值需要经过W1以及W2的正则化

**梯度**
这部分应该可以算是整个作业的最难点了

要求这里的梯度 很明显我们要使用反向传播 通过链式法则求偏导 一步步得到dW1以及dW2

$$
d\mathbf{W}_1 = \frac{\partial \mathcal{L}}{\partial \mathbf{W}_1} = \delta_2 \mathbf{a}_1^T
$$

$$
d\mathbf{W}_2 = \frac{\partial \mathcal{L}}{\partial \mathbf{W}_2} = \delta_3 \mathbf{a}_2^T
$$

用人话来说 就是通过偏导公式先得到dW1(正则化)

然后需要获得hidden层的dscore 然后反向通过激活函数 最后以hidden层的dscore推倒出dW1

**注意**这里的每一步都需要正则化（激活函数部分除外）

最后将梯度存入字典即可

这样以来 神经网络的大体框架就出来了

# 最后 调整超参数 
一个几乎在每个任务中都会出现的部分 实现并不难 也就是通过循环 使用不同的参数来调用solver 最终得到不同的效果









